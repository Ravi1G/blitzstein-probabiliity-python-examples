{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Markov chain Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief summary\n",
    "\n",
    "### Metropolis-Hastings\n",
    "\n",
    "Let $\\boldsymbol{s} = (s_1, ..., s_M)$ be a desired stationary distribution on state space ${1, ..., M}$. Assume that $s_i > 0$ for all $i$ (if not, just delete any states $i$ with $s_i=0$ from the state space). Suppose that $P = (p_{ij})$ is the transition matrix for a Markov chain on state space ${1, ..., M}$. Intuitively, $P$ is a Markov chain that we know how to run but that doesn't have the desired stationary distribution.\n",
    "\n",
    "Our goal is to modify $P$ to construct a Markov chain $X_0, X_1, ...$ with stationary distribution $\\boldsymbol{s}$. We will give a Metropolis-Hastings algorithm for this. Start at any state $X_0$ (chosen randomly or deterministically), and suppose that the new chain is currently at $X_n$. To make one of the new chain, do the following.\n",
    "\n",
    "1. If $X_n = i$, propose a new state $j$ using the transition probabilities in the $i$th row of the original transition matrix $P$.\n",
    "2. Compute the *acceptance probability*\n",
    "\\begin{equation}\n",
    "a_{ij} = min\\big( \\frac{s_jp_{ji}} {s_ip_{ij}}, 1 \\big)\n",
    "\\end{equation}\n",
    "3. Flip a coin that lands Heads with probability $a_{ij}$.\n",
    "4. If the coin lands Heads, accept the proposal (i.e., go to $j$), setting $X_{n+1} = j$. Otherwise, reject the proposal (i.e., stay at $i$), setting $X_{n+1}=i$.\n",
    "\n",
    "- Both $\\boldsymbol{s}$ and $P$ are very general, and nothing was stipulated about their being related. In practice, however, the choice of the proposal distribution is extremely important since it can make an enormous difference in how *fast* the chain converges to its stationary distribution.\n",
    "\n",
    "\n",
    "### Gibbs sampling\n",
    "\n",
    "#### Systematic scan Gibbs sampler\n",
    "\n",
    "Let $X$ and $Y$ be discrete r.v.s with joint PMF $p_{X,Y}(x,y) = P(X=x, Y=y)$. We wish to construct a two-dimensional Markov chain $(X_n, Y_n)$ whose stationary distribution is $p_{X,Y}$. The systematic scan Gibbs sampler proceeds by updating the X-component and the Y-component in alternation. If the current state is $(X_n,Y_n)=(x_n,y_n)$, then we update the X-component while holding the Y-component fixed, and then update the Y-component while holding the X-component fixed:\n",
    "\n",
    "1. Draw a value $x_{n+1}$ from the conditional distribution of $X$ given $Y=y_n$, and set $X_{n+1} = x_{n+1}$.\n",
    "2. Draw a value $y_{n+1}$ from the conditional distribution of $Y$ given $X=x_{n+1}$, and set $Y_{n+1} = y_{n+1}$.\n",
    "\n",
    "Repeating steps 1 and 2 over and over, the stationary distribution of the chain $(X_0,Y_0)$, $(X_1,Y_1)$, $(X_2,Y_2)$, $...$ is $p_{X,Y}$.\n",
    "\n",
    "#### Random scan Gibbs sampler\n",
    "\n",
    "Let $X$ and $Y$ be discrete r.v.s with joint PMF $p_{X,Y}(x,y) = P(X=x, Y=y)$. We wish to construct a two-dimensional Markov chain $(X_n, Y_n)$ whose stationary distribution is $p_{X,Y}$. Each move of the random scan Gibbs sampler picks a uniformly random component and updates it, according to the conditional distribution given the other component:\n",
    "\n",
    "1. Choose which component to update, with equal probabilities.\n",
    "2. If the X-component was chosen, draw a value $x_{n+1}$ from the conditional distribution of $X$ given $Y=y_n$, and set $X_{n+1} = x_{n+1}$. $Y_{n+1}=y_n$. Similarly, if the Y-component was chosen, draw a value $y_{n+1}$ from the conditional distribution of $Y$ given $X=x_n$, and set $X_{n+1}=x_n$, $Y_{n+1} = y_{n+1}$.\n",
    "\n",
    "Repeating steps 1 and 2 over and over, the stationary distribution of the chain $(X_0,Y_0)$, $(X_1,Y_1)$, $(X_2,Y_2)$, $...$ is $p_{X,Y}$.\n",
    "\n",
    "Gibbs sampling generalizes naturally to higher dimensions. If we want to sample from a $d$-dimensional joint distribution, the Markov chain we construct will be a sequence of $d$-dimensional random vectors. At each stage, we choose one component of the vector to update, and we draw from the conditional distribution of that component given the most recent values of the other components. We can either cycle through the components of the vector in a systematic order, or choose a random component to update each time.\n",
    "\n",
    "#### Random scan Gibbs as Metropolis-Hastings\n",
    "\n",
    "The random scan Gibbs sampler is a special case of the Metropolis-Hastings algorithm, in which the proposal is *always* accepted. In particular, it follows that the stationary distribution of the random scan Gibbs sampler is as desired.\n",
    "\n",
    "\n",
    "## Python examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, binom, beta, poisson\n",
    "from numpy.random import choice\n",
    "from numpy.linalg import matrix_power\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metropolis-Hastings\n",
    "\n",
    "#### Normal-Normal conjugacy\n",
    "\n",
    "Let $Y|\\theta \\sim \\mathcal{N}(\\theta, \\sigma^2)$, where $\\sigma^2$ is known but $\\theta$ is unknown. Using the Bayesian framework, we treat $\\theta$ as a random variable, with prior given by $\\theta \\sim \\mathcal{N}(\\mu, \\tau^2)$ for some known constants $\\mu$ and $\\tau^2$. That is, we have the two-level model\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta \\sim \\mathcal{N}(\\mu, \\tau^2) \\\\\n",
    "Y|\\theta \\sim \\mathcal{N}(\\theta, \\sigma^2).\n",
    "\\end{equation}\n",
    "\n",
    "Describe how to use the Metropolis-Hastings algorithm to find the posterior mean and variance of $\\theta$ after observing the value of $Y$.\n",
    "\n",
    "*Solution:*\n",
    "After observing $Y=y$, we can update our prior uncertainty for $\\theta$ using Bayes' rule. Because we are interested in the posterior distribution of $\\theta$, any terms not depending on $\\theta$ can be treated as part of the normalizing constant. Thus,\n",
    "\n",
    "\\begin{equation}\n",
    "f_{\\theta|Y}(\\theta|y) \\propto f_{Y|\\theta}(y|\\theta)f_{\\theta}(\\theta) \n",
    "\\propto e^{-\\frac{1}{2\\sigma^2} (y-\\theta)^2} e^{-\\frac{1}{2\\tau^2} (\\theta-\\mu)^2}.\n",
    "\\end{equation}\n",
    "\n",
    "Since we have a quadratic function of $\\theta$ in the exponent, we recognize the posterior PDF of $\\theta$ as a Normal PDF. The posterior distribution stays in the Normal family, which tells us that *the Normal is the conjugate prior of the Normal*. In fact, by completing the square (a rather tedious calculation which we shall omit), we can obtain an explicit formula for the posterior distribution of $\\theta$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta|Y=y \\sim \\mathcal{N}\\big( \\frac{\\frac{1}{\\sigma^2}}{\\frac{1}{\\sigma^2} + \\frac{1}{\\tau^2}}y + \\frac{\\frac{1}{\\tau^2}}{\\frac{1}{\\sigma^2} + \\frac{1}{\\tau^2}}\\mu, \\frac{1}{\\frac{1}{\\sigma^2} + \\frac{1}{\\tau^2}} \\big).\n",
    "\\end{equation}\n",
    "\n",
    "Let's suppose we didn't know how to complete the square, or that we wanted to check our calculations for specific values of $y$, $\\sigma^2$, $\\mu$, and $\\tau^2$. We can do this by simulating from the posterior distribution of $\\theta$, using the Metropolis-Hastings algorithm to construct a Markov chain whose stationary distribution is $f_{\\theta|Y}(\\theta|y)$. A Metropolis-Hastings algorithm for generating $\\theta_0, \\theta_1, ...$ is as follows.\n",
    "\n",
    "1. If $\\theta_n=x$, propose a new state $x'$ according to some transition rule. One way to do this in a continuous state space is to generate a Normal r.v. $\\epsilon_n$ with mean $0$ and add it onto the current state to get the proposed state: in other words, we generate $\\epsilon_n \\sim \\mathcal{N}(0, d^2)$ for some constant $d$, and then set $x' = x + \\epsilon_n$. This is the analog of a transition matrix for a continuous state space. The only additional detail is deciding $d$; in practice, we try to choose a moderate value that is neither too large nor too small.\n",
    "2. The acceptance probability is\n",
    "\\begin{equation}\n",
    "a(x,x') = min\\big(\\frac{\\boldsymbol{s}(x')p(x',x)} {\\boldsymbol{s}(x)p(x,x')}, 1\\big),\n",
    "\\end{equation}\n",
    "where $\\boldsymbol{s}$ is the desired stationary PDF (this was a PMF in the discrete case) and $p(x, x')$ is the probability *density* of proposing $x'$ from $x$ (this was $p_{ij}$ in the discrete case).\n",
    "In this problem, we want the stationary PDF to be $f_{\\theta|Y}$, so we'll use that for $\\boldsymbol{s}$. As for $p(x,x')$, proposing $x'$ from $x$ is the same as having $\\epsilon_n = x'-x$, so we evaluate the PDF of $\\epsilon_n$ at $x'-x$ to get\n",
    "\\begin{equation}\n",
    "p(x,x') = \\frac{1}{\\sqrt{2\\pi}d} e^{-\\frac{1}{2d^2}(x'-x)^2A.}\n",
    "\\end{equation}\n",
    "However, since $p(x',x) = p(x,x')$, these terms cancel from the acceptance probability, leaving us with\n",
    "\\begin{equation}\n",
    "a(x,x') = min\\big( \\frac{f_{\\theta|Y}(x'|y)}{f_{\\theta|Y}(x|y)}, 1 \\big).\n",
    "\\end{equation}\n",
    "3. Flip a coin that lands Heads with probability $a(x,x')$, independently of the Markov chain.\n",
    "4. If the coin lands Heads, accept the proposal and set $\\theta_{n+1}=x'$. Otherwise, stay in place and set $\\theta_{n+1}=x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose our observed value of Y \n",
    "# and decide on values for the constants \\theta, \\mu, and \\tau:\n",
    "y = 3\n",
    "sigma = 1\n",
    "mu = 0 \n",
    "tau = 2\n",
    "d = 1\n",
    "niter = 10**4\n",
    "Theta = np.zeros(niter, dtype=np.float)   # allocate a vector Theta of length niter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize \\theta to the observed value y, then run the algorithm:\n",
    "Theta[0] = y\n",
    "for i in range(1, niter):\n",
    "    theta_new = Theta[i-1] + norm.rvs(scale=d, size=1)\n",
    "    # Compute the acceptance probability:\n",
    "    r = norm.pdf(y, loc=theta_new, scale=sigma) * norm.pdf(theta_new, loc=mu, scale=tau) / \\\n",
    "         (norm.pdf(y, loc=Theta[i-1], scale=sigma) * norm.pdf(Theta[i-1], loc=mu, scale=tau))\n",
    "    flip = binom.rvs(n=1, p=min(r, 1), size=1)\n",
    "    Theta[i] = theta_new if flip == 1 else Theta[i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discard some of the initial draws \n",
    "# to give the chain some time to approach the stationary distribution:\n",
    "Theta_latter = Theta[niter/2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram\n",
    "plt.figure(figsize=(10, 5))\n",
    "_ = plt.hist(Theta_latter, bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs\n",
    "\n",
    "#### Chicken-egg with unknown parameters\n",
    "\n",
    "A chicken lays $N$ eggs, where $N \\sim Pois(\\lambda)$. Each egg hatches with probability $p$, where $p$ is unknown; we let $p \\sim Beta(a,b)$. The constants $\\lambda, a, b$ are known.\n",
    "\n",
    "We don't get to observe $N$. Instead, we only observe the number of eggs that hatch, $X$. Describe how to use Gibbs sampling to find $E(p|X=x)$, the posterior mean of $p$ after observing $x$ hatched eggs.\n",
    "\n",
    "*Solution:*\n",
    "\n",
    "By the chicken-egg story, the distribution of $X$ given $p$ is $Pois(\\lambda p)$. The posterior PDF of $p$ is proportional to\n",
    "\n",
    "\\begin{equation}\n",
    "f(p|X=x) \\propto P(X=x|p)f(p) \\propto e^{-\\lambda p}(\\lambda p)^xp^{a-1}q^{b-1},\n",
    "\\end{equation}\n",
    "\n",
    "where we have dropped all terms not depending on $p$.\n",
    "\n",
    "Conditional on observing $N=n$ and knowing the true value of $p$, the distribution of $X$ would be $Bin(n,p)$. By conditioning on the total number of eggs, we recover *Beta-Binomial conjugacy* between $p$ and $X$:\n",
    "\n",
    "\\begin{equation}\n",
    "p|X=x, N=n \\sim Beta(x+a, n-x+b).\n",
    "\\end{equation}\n",
    "\n",
    "We make an initial guess for $p$ and $N$, then iterate the following steps:\n",
    "\n",
    "1. Conditional on $N=n$ and $X=x$, draw a new guess for $p$ from the $Beta(x+a, n-x+b)$ distribution.\n",
    "2. Conditional on $p$ and $X=x$, the number of unhatched eggs is $Y \\sim Pois(\\lambda(1-p))$ by the chicken-egg story, so we can draw $Y$ from the $Pois(\\lambda(1-p))$ distribution and set the new guess for $N$ to be $N=x+Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decide the observed value of X, as well as the constants \\lambda, a, b:\n",
    "x = 7\n",
    "l = 10\n",
    "a = 1\n",
    "b = 1\n",
    "niter = 10**4\n",
    "P = np.zeros(niter, dtype=np.float)\n",
    "N = np.zeros(niter, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize p and N to the values 0.5 and 2x, respectively,\n",
    "# then run the algorithm:\n",
    "P[0] = 0.5\n",
    "N[0] = 2*x\n",
    "for i in range(1, niter):\n",
    "    P[i] = beta.rvs(a=x+a, b=N[i-1]-x+b)\n",
    "    N[i] = x + poisson.rvs(mu=l*(1-P[i-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discard some of the initial draws \n",
    "# to give the chain some time to approach the stationary distribution:\n",
    "P_latter = P[niter/2:]\n",
    "N_latter = N[niter/2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram\n",
    "plt.figure(figsize=(10, 10))\n",
    "fig, axes = plt.subplots(2, 1)\n",
    "_ = axes[0].hist(P_latter, bins=100)\n",
    "_ = axes[1].hist(N_latter, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
